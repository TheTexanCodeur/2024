








# Import libraries
import requests
from bs4 import BeautifulSoup














# Make the request
r = requests.get('https://httpbin.org/ip') # /ip: Returns the requester's IP Address.

print('Response status code: {0}\n'.format(r.status_code))
print('Response headers: {0}\n'.format(r.headers))
print('Response body: {0}'.format(r.text))





r = requests.get('http://worldtimeapi.org/api/timezone/Europe/Zurich')

print('Response body (parsed json):')
r.json()





r = requests.get('https://httpbin.org/get?key1=value1')
r.json()





payload = {'key1': 'value1', 'key2': 'value2'}
r = requests.post('https://httpbin.org/post', data=payload)
r.json()








# Send the request
r = requests.get('https://httpbin.org/html')
r.text[:300]





# Extract the header
soup = BeautifulSoup(r.text, 'html.parser')
soup.h1





URL = 'http://dblp.uni-trier.de/pers/hd/v/Vetterli:Martin'





r = requests.get(URL)
page_body = r.text





page_body[:300]





soup = BeautifulSoup(page_body, 'html.parser')





soup.title





soup.title.string





all_links = soup.find_all('a')
print('The webpage cointains {0} links...'.format(len(all_links)))


external_links = 0
for link in all_links:
    if(link.get('href') and not link.get('href').startswith('http://dblp.uni-trier.de/')
       and link.get('href').startswith('http')):  # just an example, you may need more checks
        external_links += 1

print('... and {0} of them point to external websites.'.format(external_links))





publications_wrappers = soup.find_all('li', class_='entry')





print('Total number of items: {0}'.format(len(publications_wrappers)))


for p in publications_wrappers:
    print(p.find('span', class_='title').text)


import pandas as pd
%matplotlib inline


publications_list = []
for p in publications_wrappers:
    title = p.find('span', class_='title').text  # get the title
    authos_list = p.find_all('span', {'itemprop': 'author'})  # get the authors list
    authors = [author.text for author in authos_list]  
    year = p.find('span', {'itemprop': 'datePublished'}).text
    publications_list.append({'title': title, 
                         'authors': authors, 
                         'year': int(year)})  # here you should validate the data

publications = pd.DataFrame.from_dict(publications_list)
publications.head()


publications.groupby('year')\
    .count()\
    .rename(columns = {'title':'count'})\
    .plot(y='count', kind='bar', grid=True, figsize=(10, 6), title='Data from: ' + URL)
